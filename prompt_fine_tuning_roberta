# Prompt v1

Genera un **script en Python bien documentado** que implemente el **fine-tuning del modelo RoBERTa** para an√°lisis de sentimientos en rese√±as de texto.

El script debe estar dividido en secciones claras con comentarios y debe incluir todo el flujo de trabajo, en este orden:

1. **Importaciones necesarias**
   - PyTorch, Hugging Face Transformers, scikit-learn, datasets, etc.

2. **Carga y exploraci√≥n de datos**
   - Dataset de rese√±as con etiquetas `positive` y `negative`.
   - Conversi√≥n de etiquetas a valores num√©ricos.
   - Divisi√≥n en train y test (80/20).

3. **Tokenizaci√≥n con RoBERTa**
   - Uso de `RobertaTokenizer` (`roberta-base`).
   - Configuraci√≥n de `MAX_LEN`.
   - Conversi√≥n de textos a tensores (`input_ids`, `attention_mask`, `token_type_ids`).

4. **Creaci√≥n de dataset y dataloader**
   - Implementaci√≥n de la clase `SentimentData`.
   - Configuraci√≥n de `DataLoader` para entrenamiento y validaci√≥n.

5. **Definici√≥n del modelo**
   - Clase `RobertaClass` que usa `RobertaModel.from_pretrained("roberta-base")`.
   - Capas adicionales: lineal intermedia (768‚Üí768), ReLU, dropout (0.2), salida lineal (768‚Üí2).

6. **Configuraci√≥n de entrenamiento**
   - Funci√≥n de p√©rdida: `CrossEntropyLoss`.
   - Optimizador: `Adam` con `learning_rate=1e-5`.
   - Funciones de entrenamiento (`train`) y validaci√≥n (`valid`) con m√©tricas de precisi√≥n y f1.

7. **Entrenamiento del modelo**
   - Ejecutar el fine-tuning por 1 √©poca.
   - Mostrar m√©tricas intermedias y finales.

8. **Evaluaci√≥n final**
   - Calcular `accuracy`, `precision`, `recall`, `f1-score` sobre el test set.
   - Mostrar `classification_report`.

9. **Guardado y carga del modelo entrenado**
   - Guardar el modelo y el tokenizer.
   - Funci√≥n `predict(text)` para clasificar nuevas rese√±as.

10. **Ejemplo de uso**
    - Pasar una lista de rese√±as nuevas al modelo y mostrar si son `positive` o `negative`.

üîπ El c√≥digo debe estar organizado, modular y comentado paso a paso para que cualquier persona pueda entenderlo y ejecutarlo en Google Colab o localmente.

